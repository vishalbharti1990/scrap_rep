{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using deep feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input as prpc_vgg_res\n",
    "from keras.applications.xception import preprocess_input as prpc_xce_inc\n",
    "\n",
    "from keras.models import Model\n",
    "import keras.applications.vgg16 as vgg16\n",
    "import keras.applications.vgg19 as vgg19\n",
    "import keras.applications.resnet50 as res\n",
    "import keras.applications.inception_v3 as inc\n",
    "import keras.applications.xception as xce\n",
    "\n",
    "import pickle\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# This is the feature extraction module/class\n",
    "class Feature_extractor:\n",
    "\tdef __init__(self, dataset, cnn_architecture, layer):\n",
    "\t\t\n",
    "\t\tself.dataset_path = \"D:/clus_data/Data/%s/\" % dataset\n",
    "\t\tself.dataset_im_path = self.dataset_path + \"Images/\"\n",
    "\t\tprint(self.dataset_im_path)\n",
    "\t\tself.dataset_feat_path = self.dataset_path + \"Features/%s_%s\" % (cnn_architecture, layer)\n",
    "\n",
    "\t\text_file = open( self.dataset_path + \"extension.txt\", \"r\")\n",
    "\t\tself.extension = ext_file.read()\n",
    "\t\text_file.close()\n",
    "\t\t\n",
    "\t\tself.n_files = len(glob(self.dataset_im_path + \"*\" + self.extension))\n",
    "\t\tprint(self.n_files)\n",
    "\t\tself.network_name = cnn_architecture\n",
    "\t\tself.layer_name   = layer\n",
    "\t\t\n",
    "\t\tself.get_network_characteristics()\n",
    "\t\tprint(\"Feature extractor: %s // %s\" % (self.network_name, self.layer_name))\n",
    "\n",
    "\tdef get_network_characteristics(self):\n",
    "\t\tif self.network_name == \"vgg16\":\n",
    "\t\t\tbase_model = vgg16.VGG16(weights='imagenet')\n",
    "\t\t\tself.model = Model(inputs=base_model.input, outputs=base_model.get_layer(self.layer_name).output)\n",
    "\t\t\tself.tgt_size = (224, 224)\n",
    "\t\t\tself.prpc_fct = prpc_vgg_res\n",
    "\t\t\t\n",
    "\t\telif self.network_name == \"vgg19\":\n",
    "\t\t\tbase_model = vgg19.VGG19(weights='imagenet')\n",
    "\t\t\tself.model = Model(inputs=base_model.input, outputs=base_model.get_layer(self.layer_name).output)        \n",
    "\t\t\tself.tgt_size = (224, 224)\n",
    "\t\t\tself.prpc_fct = prpc_vgg_res\n",
    "\t\t\t\n",
    "\t\telif self.network_name == \"resnet\":\n",
    "\t\t\tbase_model = res.ResNet50(weights='imagenet')\n",
    "\t\t\tself.model = Model(inputs=base_model.input, outputs=base_model.get_layer(self.layer_name).output)        \n",
    "\t\t\tself.tgt_size = (224, 224)\n",
    "\t\t\tself.prpc_fct = prpc_vgg_res\n",
    "\t\t\t\n",
    "\t\telif self.network_name == \"xception\":\n",
    "\t\t\tbase_model = xce.Xception(weights='imagenet')\n",
    "\t\t\tself.model = Model(inputs=base_model.input, outputs=base_model.get_layer(self.layer_name).output)        \n",
    "\t\t\tself.tgt_size = (299, 299)\n",
    "\t\t\tself.prpc_fct = prpc_xce_inc\n",
    "\t\t\t\n",
    "\t\telif self.network_name == \"inception\":\n",
    "\t\t\tbase_model = inc.InceptionV3(weights='imagenet')\n",
    "\t\t\tself.model = Model(inputs=base_model.input, outputs=base_model.get_layer(self.layer_name).output)\n",
    "\t\t\tself.tgt_size = (299, 299)\n",
    "\t\t\tself.prpc_fct = prpc_xce_inc\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Error: possible network names:\\n-'vgg16'\\n-'vgg19'\\n-'resnet'\\n-'xception'\\n-'inception'\")\n",
    "\t\t\t\n",
    "\tdef get_PIL_image(self, image_name):\n",
    "\t\tfile_name = self.dataset_im_path + image_name + self.extension\n",
    "\t\t\n",
    "\t\treturn image.load_img(file_name, target_size = self.tgt_size)\n",
    "\t\n",
    "\tdef get_arr_image(self, image_name):\n",
    "\t\tpil_im = self.get_PIL_image(image_name)\n",
    "\t\n",
    "\t\treturn image.img_to_array(pil_im)\n",
    "\t\n",
    "\tdef get_prpc_image(self, image_name):\n",
    "\t\tarr_im = self.get_arr_image(image_name)\n",
    "\t\tarr_im = np.expand_dims(arr_im, axis = 0)\n",
    "\t\t\n",
    "\t\treturn self.prpc_fct(arr_im)\n",
    "\t\n",
    "\tdef extract(self, image_name):\n",
    "\t\tprpc_im = self.get_prpc_image(image_name)\n",
    "\t\t\n",
    "\t\treturn np.ndarray.flatten(self.model.predict(prpc_im))\n",
    "\n",
    "\tdef extract_and_save_features(self):\n",
    "\t\tif not os.path.exists(self.dataset_feat_path):\n",
    "\t\t\tos.makedirs(self.dataset_feat_path)\n",
    "\t\telse:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tprint(\"extracting and saving features ... \")\n",
    "\t\t\n",
    "\t\tgc.collect()\n",
    "        \n",
    "\t\tfor im in range(self.n_files):\n",
    "\t\t\tif im % 100 == 0:\n",
    "\t\t\t\tprint(\"    %d/%d\" % (im, self.n_files))\n",
    "\t\t\tfeatures = self.extract(str(im))\n",
    "\t\t\tfeat_file = open(self.dataset_feat_path + \"/%s.p\" % im, \"wb\")\n",
    "\t\t\tpickle.dump(features, feat_file)\n",
    "\t\t\tfeat_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for clustering evaluation\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def confusion_matrix(clusters, classes_gt):\n",
    "\tnew_gt = deepcopy(classes_gt)\n",
    "\tl = list(set(classes_gt))\n",
    "\tfor i in range(len(classes_gt)):\n",
    "\t\tfor j in range(len(l)):\n",
    "\t\t\tif classes_gt[i] == l[j]:\n",
    "\t\t\t\tnew_gt[i] = j\n",
    "\t\t\t\t\n",
    "\tconf_mat = np.zeros([len(set(clusters)), len(set(new_gt))])\n",
    "\tfor i in range(len(clusters)):\n",
    "\t\tconf_mat[clusters[i], new_gt[i]] += 1\n",
    "\n",
    "\treturn conf_mat\n",
    "\n",
    "def purity(clusters, classes_gt):\n",
    "\tconf_mat = confusion_matrix(clusters, classes_gt)\n",
    "\tsum_clu  = np.max(conf_mat, axis = 1)\n",
    "\tsum_tot  = np.sum(sum_clu)\n",
    "\n",
    "\tpur = sum_tot / len(clusters)\n",
    "\n",
    "\treturn pur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MiniBatchKMeans, MeanShift, AffinityPropagation, Birch, DBSCAN\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "\n",
    "# The clustering module\n",
    "class Clusterer:\n",
    "\tdef __init__(self, dataset, cnn_architecture, layer, clustering_algorithm, n_classes = 0):\n",
    "\t\tself.dataset_path = \"D:/clus_data/Data/%s/\" % dataset\n",
    "\t\tself.dataset_feat_path = self.dataset_path + \"Features/%s_%s/\" % (cnn_architecture, layer)\n",
    "\n",
    "\t\tself.n_files = len(glob(self.dataset_feat_path + \"*.p\"))\n",
    "\t\t\n",
    "\t\tself.get_n_classes(n_classes)\n",
    "\t\tprint(\"Number of classes: %d\" % self.n_classes)\n",
    "\t\t\n",
    "\t\tself.get_algorithm(clustering_algorithm)\n",
    "\t\tprint(\"Algorithm: \" + str(self.algorithm).split(\"(\")[0])\n",
    "\t\t\n",
    "\t\tself.get_features()\n",
    "\t\tprint(\"Features shape: \" + str(self.features.shape))\n",
    "\t\t\t\n",
    "\tdef get_n_classes(self, n_classes):\n",
    "\t\tif os.path.exists(self.dataset_path + \"true_labels.txt\"):\n",
    "\t\t\ttrue_lab_file = open(self.dataset_path + \"true_labels.txt\", \"r\")\n",
    "\t\t\tself.true_labels = [int(tl.rstrip(\"\\n\")) for tl in true_lab_file.readlines()]\n",
    "\t\t\ttrue_lab_file.close()\n",
    "\n",
    "\t\t\tself.n_classes = len(list(set(self.true_labels)))\n",
    "\t\telif n_classes != 0:\n",
    "\t\t\tself.n_classes = n_classes\n",
    "\t\t\tself.true_labels = 0\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Error: %s folder must contain a true_labels.txt file OR n_classes must be a positive integer\" % self.dataset_path)\n",
    "\t\t\treturn\n",
    "\t\n",
    "\tdef get_algorithm(self, clustering_algorithm):\n",
    "\t\tif clustering_algorithm == \"kmeans\":\n",
    "\t\t\tself.algorithm = KMeans(self.n_classes, precompute_distances=False)\n",
    "\t\telif clustering_algorithm == \"mb_kmeans\":\n",
    "\t\t\tself.algorithm = MiniBatchKMeans(self.n_classes)\n",
    "\t\telif clustering_algorithm == \"affinity_prop\":\n",
    "\t\t\tself.algorithm = AffinityPropagation()\n",
    "\t\telif clustering_algorithm == \"mean_shift\":\n",
    "\t\t\tself.algorithm = MeanShift()\n",
    "\t\telif clustering_algorithm == \"agglomerative\":\n",
    "\t\t\tself.algorithm = AgglomerativeClustering(self.n_classes)\n",
    "\t\telif clustering_algorithm == \"birch\":\n",
    "\t\t\tself.algorithm = Birch(self.n_classes)\n",
    "\t\telif clustering_algorithm == \"dbscan\":\n",
    "\t\t\tself.algorithm = DBSCAN()\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Error: This clustering algorithm is not available. Choose among the following options: 'kmeans', 'mb_kmeans', 'affinity_prop', 'mean_shift', 'agglomerative', 'birch', 'dbscan'\")\n",
    "\t\t\n",
    "\tdef get_features(self):\n",
    "\t\tself.features = []\n",
    "\t\tfor i in range(self.n_files):\n",
    "\t\t\tfile = open(self.dataset_feat_path + \"%d.p\" % i, \"rb\")\n",
    "\t\t\tself.features.append(pickle.load(file))\n",
    "\t\t\tfile.close()\n",
    "\t\tself.features = np.array(self.features)\n",
    "\t\n",
    "\tdef cluster(self):\n",
    "\t\tprint(\"Clustering ...\")\n",
    "\t\tself.predicted_labels = self.algorithm.fit_predict(self.features)\n",
    "\t\n",
    "\tdef evaluate(self, metric):\n",
    "\t\tif self.true_labels == 0:\n",
    "\t\t\tprint(\"Error: A true_labels.txt file is needed\")\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tif metric == \"nmi\":\n",
    "\t\t\tprint(\"NMI: %f\" % nmi(self.true_labels, self.predicted_labels))\n",
    "\t\telif metric == \"purity\":\n",
    "\t\t\tprint(\"Purity: %f\" % purity(self.true_labels, self.predicted_labels))\n",
    "\t\telif metric == \"both\":\n",
    "\t\t\tprint(\"NMI: %f\" % nmi(self.true_labels, self.predicted_labels, average_method=\"arithmetic\"))\n",
    "\t\t\tprint(\"Purity: %f\" % purity(self.true_labels, self.predicted_labels))\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Error: This metric is not available. Choose among the following options: 'nmi', 'purity', 'both'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated result: Inception and Xception features give an NMI of 51%, which is a significant improvement over 15% for VGG and 28% for Resnet50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 VGG16 MiniBatch K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/cifar-10/Images/\n",
      "60000\n",
      "Feature extractor: vgg16 // fc2\n",
      "Number of classes: 10\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (60000, 4096)\n",
      "Clustering ...\n",
      "Shape predicted labels: (60000,)\n",
      "NMI: 0.158868\n",
      "Purity: 0.294167\n"
     ]
    }
   ],
   "source": [
    "# running on the cifar-10 dataset\n",
    "dataset              = \"cifar-10\"\n",
    "cnn_architecture     = \"vgg16\"\n",
    "layer \t\t\t\t = \"fc2\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe.extract_and_save_features()\n",
    "cl = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl.cluster()\n",
    "predicted_labels = cl.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "cl.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 VGG-19 MiniBatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/cifar-10/Images/\n",
      "60000\n",
      "Feature extractor: vgg19 // fc2\n",
      "Number of classes: 10\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (60000, 4096)\n",
      "Clustering ...\n",
      "Shape predicted labels: (60000,)\n",
      "------------------\n",
      "------------------\n",
      "Evaluation Results\n",
      "------------------\n",
      "NMI: 0.143904\n",
      "Purity: 0.297800\n"
     ]
    }
   ],
   "source": [
    "# running on the cifar-10 dataset\n",
    "dataset              = \"cifar-10\"\n",
    "cnn_architecture     = \"vgg19\"\n",
    "layer \t\t\t\t = \"fc2\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe.extract_and_save_features()\n",
    "cl = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl.cluster()\n",
    "predicted_labels = cl.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "print(\"------------------\")\n",
    "print(\"------------------\")\n",
    "print(\"Evaluation Results\")\n",
    "print(\"------------------\")\n",
    "cl.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Resnet50 MiniBatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/cifar-10/Images/\n",
      "60000\n",
      "Feature extractor: resnet // avg_pool\n",
      "Number of classes: 10\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (60000, 2048)\n",
      "Clustering ...\n",
      "Shape predicted labels: (60000,)\n",
      "------------------\n",
      "------------------\n",
      "Evaluation Results\n",
      "------------------\n",
      "NMI: 0.268808\n",
      "Purity: 0.409367\n"
     ]
    }
   ],
   "source": [
    "# running on the cifar-10 dataset\n",
    "dataset              = \"cifar-10\"\n",
    "cnn_architecture     = \"resnet\"\n",
    "layer \t\t\t\t = \"avg_pool\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe.extract_and_save_features()\n",
    "cl = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl.cluster()\n",
    "predicted_labels = cl.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "print(\"------------------\")\n",
    "print(\"------------------\")\n",
    "print(\"Evaluation Results\")\n",
    "print(\"------------------\")\n",
    "cl.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Xception MiniBatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/cifar-10/Images/\n",
      "60000\n",
      "Feature extractor: xception // avg_pool\n",
      "Number of classes: 10\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (60000, 2048)\n",
      "Clustering ...\n",
      "Shape predicted labels: (60000,)\n",
      "------------------\n",
      "------------------\n",
      "Evaluation Results\n",
      "------------------\n",
      "NMI: 0.511780\n",
      "Purity: 0.679817\n"
     ]
    }
   ],
   "source": [
    "# running on the cifar-10 dataset\n",
    "dataset              = \"cifar-10\"\n",
    "cnn_architecture     = \"xception\"\n",
    "layer \t\t\t\t = \"avg_pool\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe.extract_and_save_features()\n",
    "cl = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl.cluster()\n",
    "predicted_labels = cl.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "print(\"------------------\")\n",
    "print(\"------------------\")\n",
    "print(\"Evaluation Results\")\n",
    "print(\"------------------\")\n",
    "cl.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Inception MiniBatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/cifar-10/Images/\n",
      "60000\n",
      "Feature extractor: inception // avg_pool\n",
      "Number of classes: 10\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (60000, 2048)\n",
      "Clustering ...\n",
      "Shape predicted labels: (60000,)\n",
      "------------------\n",
      "------------------\n",
      "Evaluation Results\n",
      "------------------\n",
      "NMI: 0.518607\n",
      "Purity: 0.662533\n"
     ]
    }
   ],
   "source": [
    "# running on the cifar-10 dataset\n",
    "dataset              = \"cifar-10\"\n",
    "cnn_architecture     = \"inception\"\n",
    "layer \t\t\t\t = \"avg_pool\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe.extract_and_save_features()\n",
    "cl = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl.cluster()\n",
    "predicted_labels = cl.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "print(\"------------------\")\n",
    "print(\"------------------\")\n",
    "print(\"Evaluation Results\")\n",
    "print(\"------------------\")\n",
    "cl.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running their modeule on COIL-100 dataset (which they used in their paper), the result is very good. NMI is 91%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/clus_data/Data/coil-100/Images/\n",
      "7200\n",
      "Feature extractor: vgg16 // fc2\n",
      "Number of classes: 100\n",
      "Algorithm: MiniBatchKMeans\n",
      "Features shape: (7200, 4096)\n",
      "Clustering ...\n",
      "Shape predicted labels: (7200,)\n",
      "NMI: 0.910459\n",
      "Purity: 0.842083\n"
     ]
    }
   ],
   "source": [
    "dataset              = \"coil-100\"\n",
    "cnn_architecture     = \"vgg16\"\n",
    "layer \t\t\t\t = \"fc2\"\n",
    "clustering_algorithm = \"mb_kmeans\"\n",
    "metric\t\t\t\t = \"both\"\n",
    "\n",
    "fe2 = Feature_extractor(dataset, cnn_architecture, layer)\n",
    "fe2.extract_and_save_features()\n",
    "cl2 = Clusterer(dataset, cnn_architecture, layer, clustering_algorithm)\n",
    "cl2.cluster()\n",
    "predicted_labels = cl2.predicted_labels\n",
    "print(\"Shape predicted labels: %s\" % str(predicted_labels.shape))\n",
    "cl2.evaluate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is my code, where I tried to replicate their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "\n",
    "model = VGG16(weights='imagenet')\n",
    "feat_extract_model = Model(inputs=model.input, outputs=model.get_layer('fc2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7200 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_gen = data_gen.flow_from_directory('D:/clus_data/Data/coil-100/Images/',target_size=(224,224), color_mode=\"rgb\", class_mode=None, shuffle=False, batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 144s 718ms/step\n"
     ]
    }
   ],
   "source": [
    "extract_feats = feat_extract_model.predict_generator(test_gen, steps=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "predictions = MiniBatchKMeans(n_clusters=100).fit_predict(extract_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"D:/clus_data/Data/coil-100/true_labels.txt\"):\n",
    "    true_lab_file = open(\"D:/clus_data/Data/coil-100/true_labels.txt\", \"r\")\n",
    "    true_labels = [int(tl.rstrip(\"\\n\")) for tl in true_lab_file.readlines()]\n",
    "    true_lab_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My approach gives an NMI of 77%, compared to their 91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Info:  0.7732649628986448\n",
      "Purity:  0.6275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "print(\"Normalized Mutual Info: \", normalized_mutual_info_score(true_labels, predictions,average_method=\"arithmetic\"))\n",
    "print(\"Purity: \", purity(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
