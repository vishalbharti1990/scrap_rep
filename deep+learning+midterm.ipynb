{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Exam : CSC 84020 Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "(1). Briefly describe the key advantages of\n",
    " * Batch Normalization\n",
    " * Self-Normalizing Activations\n",
    " * Max Norm Weight Constraints\n",
    " \n",
    " (a) Advantages of Batch Normalization:\n",
    "  * Minimizes the effects due to 'internal covariant shift'. 'Covariate shift' is defined as the change in distribution of the input to a learning algorithm. For deep neural nets, we have a layered structure where in each layer is affected by the previous layer and hence, even a small level of change in the input can be amplified as it goes through the layers. Since with batch normalization we keep the mean and variance of each layer fixed, we minimize this effect.\n",
    "  * The gradient is no longer dependent on the scale of the input features and hence converges faster. Some features that have a larger scale compared to other features can sometimes have larger effect on the gradient. With batch normalization all the features are normalized to the same scale and hence the training would converge faster.\n",
    "  * Regularizes the model and reduces the need for dropout, photometric distortions, local response normalization and other regularization techniques. In batch normalization we use small batches for the normalization, this can reduce the effect of outliers as the error that comes from these outliers is distributed across all the layers.\n",
    "  * As the input to each layer is normalized, we can use higher learning rates and not worry about hitting the low gradient zones in saturating nonlinear functions.\n",
    "  \n",
    "  (b) Advantages of Self-Normalizaing Activations:\n",
    "  * SNN's have an inherent normalizing effect and hence there is no need for an extenal normalization technique like batch normalization.\n",
    "  * The mapping of variance has an upper and lower bound, which prevents it from exploding or vanishing.\n",
    "  * The authors proved through results that FNN's with batch normalization take longer to train than SNN's.\n",
    "  * With SNN's, the mean and variance stays close to 0 and 1 for much deeper networks as compared to other techniques.\n",
    "  * Like Batch normalization, SNN's also have a regularization effect and hence eliminates the need for extenal regularization techniques.\n",
    "  \n",
    "  (c) Advantages of Max Norm Weight Constraints:\n",
    "   * As it is a regularization technique, it prevents the training algorithm to overfit the data as the weights are bounded by a max norm constraint.\n",
    "   * Higher learning rates can be used. As the weight vector for each neuron is bounded by a max norm constraint, we can safely use higher learning rates without worrying about the network \"exploding\".\n",
    "   * The learning converges faster. As it possible to use higher learning rates and the also since the weights are regularized, the learning would typically converge faster. It has been shown to improve the performance of stochastic gradient descent.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Problem 2\n",
    " (2). In this problem you are going to execute forward and backward modes of neural network on paper or by using a program (your   choice) for 3 iterations. The model is shown in Fig. 1. This network comprises of a hidden layer with 3 ReLU units and a squared-error loss. (Note: Please, use tanh as activation function in the output unit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of ReLu (Rectified linear unit)\n",
    "The definition of ReLu, followed by its vectorization. The ReLu function is defined as:\n",
    "\n",
    "$$f(x)=\\max(0,x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition of ReLu\n",
    "def ReLu(x):\n",
    "    if x > 0:\n",
    "        r = x\n",
    "    else:\n",
    "        r = 0\n",
    "    return r  \n",
    "\n",
    "# Vectorizing the function\n",
    "vecReLu = np.vectorize(ReLu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1\n",
    "### Initialization of the weights and the input and output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = np.array([[0.6,0.7,0],[0.01,0.43,0.88]])\n",
    "W2 = np.array([[0.02],[0.03],[0.09]])\n",
    "X = np.array([[0.75,0.8],[0.20,0.05],[-0.75,0.8],[0.20,-0.05]])\n",
    "Y = np.array([[1,1,-1,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2\n",
    "### Predict the labels for the input data and compute the loss.\n",
    "The following function forward propogates through the network to compute the predictions for a given set of weights and inputs. Given 'm' inputs, its returns 'm' predictions for each of the inputs along with the average loss which is given as:\n",
    "$$\\mathcal{L} = \\frac{1}{2} \\sum_{i=1}^m (y_{out}^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with the initlialized weights : 1.9666\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(W1,W2,X,Y):\n",
    "    # compute the weighted input to each neuron in first layer\n",
    "    S = np.matmul(X,W1)\n",
    "    # pass the output through ReLu\n",
    "    Z = vecReLu(S)\n",
    "    # compute the input to output layer\n",
    "    S_out = np.matmul(Z,W2)\n",
    "    # pass the input through tanh activation\n",
    "    Y_out = np.ndarray.flatten(np.tanh(S_out))\n",
    "    # compute the loss\n",
    "    Loss = np.sum((Y-Y_out)**2)/2\n",
    "    # return the output labels Y_out, the first layer outputs Z and the loss \n",
    "    return Y_out, Z, Loss\n",
    "\n",
    "_,_, Loss = forward_prop(W1,W2,X,Y)\n",
    "print(\"Loss with the initlialized weights : {:.4f}\".format(Loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of ReLu and tanh activations\n",
    "The derivative of ReLu activation function is $1$ for $x>0$, $0$ for $x<0$ and undefined for $x=0$. However, taking the derivate of ReLu to be $0$ or $1$ at $x=0$ is a general practice and has no effect on the performance. For this implementation we take the derivative to be $1$ for $x\\geq 0$ and $0$ otherwise. The derivate of $tanh$ is $\\frac{\\partial}{\\partial x}tanh(x) = 1 - tanh(x)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propogation\n",
    "The following function \"backward_prop\" computes the error signal $\\frac{\\partial \\mathcal{L}}{\\partial s_{out}}$ at the output layer and propogates backwards to compute the error signals $\\frac{\\partial \\mathcal{L}}{\\partial s_j}$ for each of the $j$ hidden units. It then uses these error signals to compute the weight updates $\\Delta W$(denoted by delta_W1 in the code) and $\\Delta w$ (delta_W2) for the hidden and output layer respectively. It takes as params the hidden layer weights W (denoted by W1 in the code), output layer weights w (denoted by W2), a matrix of inputs X, the learning_rate and the number of epochs (number of repetitions). Next, we run this function for 3 epochs by inputing the wieghts W1, W2, the input X and the labels Y as initialized before and using a learning rate of 0.1. It displays the updated weight and the loss at the end of each epoch. The loss is decreased after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Updated weights after epoch : 1 ####\n",
      "\n",
      "Hidden layer weight : \n",
      "\n",
      "[[ 0.62058652  0.73439707  0.05962456]\n",
      " [-0.00221834  0.40848283  0.86284415]]\n",
      "\n",
      "Output layer weight : \n",
      "\n",
      "[[ 0.06079306]\n",
      " [ 0.11165028]\n",
      " [ 0.08266383]]\n",
      "\n",
      "Loss = 1.8786 \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "#### Updated weights after epoch : 2 ####\n",
      "\n",
      "Hidden layer weight : \n",
      "\n",
      "[[ 0.65904748  0.80457115  0.1160346 ]\n",
      " [-0.01484139  0.38615561  0.84576108]]\n",
      "\n",
      "Output layer weight : \n",
      "\n",
      "[[ 0.10158612]\n",
      " [ 0.19330056]\n",
      " [ 0.07532765]]\n",
      "\n",
      "Loss = 1.7978 \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "#### Updated weights after epoch : 3 ####\n",
      "\n",
      "Hidden layer weight : \n",
      "\n",
      "[[ 0.71538288  0.91052223  0.16923013]\n",
      " [-0.02786915  0.36301833  0.82875079]]\n",
      "\n",
      "Output layer weight : \n",
      "\n",
      "[[ 0.14237918]\n",
      " [ 0.27495084]\n",
      " [ 0.06799148]]\n",
      "\n",
      "Loss = 1.7230 \n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def backward_prop(W1,W2,X,Y,learning_rate,epochs):\n",
    "    Y_out, Z, _ = forward_prop(W1,W2,X,Y)\n",
    "    \n",
    "    for e in range(0, epochs):\n",
    "        for i in range(0,X.shape[0]):\n",
    "            #compute delta_out\n",
    "            delta_out = (1-(Y_out.T[i])**2)*(Y_out.T[i] - Y.T[i])\n",
    "\n",
    "            delta_j = np.zeros((len(Z[i]),1))\n",
    "\n",
    "            for j in range(0,len(Z[i])):\n",
    "                if Z[i][j] >= 0:\n",
    "                    delta_j[j] = W2[j] * delta_out\n",
    "                else:\n",
    "                    delta_j[j] = 0\n",
    "\n",
    "\n",
    "            del_W2 = (delta_out * Z[i]).reshape(3,1)\n",
    "            #print(del_W2.shape)\n",
    "\n",
    "            del_W1 = (np.array([X[i]*3]).T * delta_j.T)\n",
    "\n",
    "            W1 = (W1 - learning_rate * del_W1)\n",
    "\n",
    "            W2 = (W2 - learning_rate * del_W2)\n",
    "            \n",
    "        print(\"\\n#### Updated weights after epoch : {:d} ####\\n\".format(e+1))\n",
    "        print(\"Hidden layer weight : \\n\")\n",
    "        print(W1)\n",
    "        print(\"\\nOutput layer weight : \\n\")\n",
    "        print(W2)\n",
    "        _,_, loss = forward_prop(W1,W2,X,Y)\n",
    "        print(\"\\nLoss = {:.4f} \\n\".format(loss))\n",
    "        print(\"----------------------------------------\")\n",
    "    \n",
    "    return W1,W2\n",
    "    \n",
    "    \n",
    "W1n, W2n = backward_prop(W1,W2,X,Y,0.1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
